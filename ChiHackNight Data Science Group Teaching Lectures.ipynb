{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiHackNight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro to Data Science Group Teaching Lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teacher Mendy Zimmerman Githib/mendyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Cleaning and Exploration and preperation\n",
    "    - train test split\n",
    "2. Model Building\n",
    "3. Model training\n",
    "4. Model Evaluation and results analyzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/\n",
    "\n",
    "Data Wrangling notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised and unsupervised ML problems\n",
    "\n",
    "Supervised ML Algorithms\n",
    "    Classification and Regression\n",
    "    \n",
    "**Classification** - Goals is to predict the Class Label. Which comes from a predefined list. Language is a classification (is a website english or Tagalog? That is a yes or no question.) \n",
    "        Binary classification - Yes or no, Spam or No Spam.\n",
    "        Multi-Class Classification - Iris example had 3.\n",
    "\n",
    "**Regression** - Goal is to predict a continuous number. A floating point number. A real number. Ask yourself if there is Continuity in the answer. i.e. annual income.\n",
    "    \n",
    "**Generalizability**\n",
    "   - Overfitting\n",
    "   - Underfitting\n",
    "   - Model complexity\n",
    "\n",
    "# SUPERVISED ML ALGORITHMS\n",
    "\n",
    "#### Lecture 1 (DONE)\n",
    "- Classifying the iris species\n",
    "- meet the data\n",
    "- measuring success for traing and test data\n",
    "- looking at your data\n",
    "- building the forst model with k-nearest neighbors\n",
    "- making the predictions\n",
    "- evaluating the model\n",
    "- classification and regression\n",
    "- generalization, overfitting, underfitting\n",
    "- relation of model complexity to dataset size\n",
    "- supervised ml algos\n",
    "- some sample synthetic data sets and the cancer and boston/boston extended data sets\n",
    "- k - nearest neighbors in classification and regression\n",
    "\n",
    "### k-Nearest Neighbors\n",
    "- Good for small datasets, good as a baseline, easy to explain what the model is doing and predicting\n",
    "\n",
    "- The k-NN algorithm is the simplist ML algo. To build the model just stores the training data set. To predict on the model with a new data point, the algo will find the closest data point in the training dataset, aka it's nearest neighbor. \n",
    "- n_neighbor=1 indicates only one neighbor and the closest is returned. You can add many, aka k in k-NN, and it pulls the mean value of the neighbors you wanted (voting to assign the label). So for n_neighbor=3 it will count how mahy classes in each category we have (perhaps 2 in class 0 and 1 in class 3) then whomever is more frequent will assigned to the new data point. \n",
    "- we can plot a fit line to visualize the boundry between the clasess. \n",
    "- synthetic Wave and Forge datasets examples are toy examples to understand what is happening\n",
    "- strength/weaknesses - easy to explain, gives reasonable performance without much adjustments, Use this before applying more complex ones as a good baseline. Building the model is fast except when using large datasets in features or samples (bc it stores everything in the dataset) then it predicts slow, You should preprocess your data (See unsupervised learning tools) to use k-NN, it does not work well on sparse datasets (data with mostly 0's). Not always used in practice bc of the slow predictions and it's inability to handle many features...move onto Linear Models to overcome these restrictions.   \n",
    "\n",
    "**Nearest Neighbors Claasification** \n",
    "- from sklearn.neighbors import KNeighborsClassifier\n",
    "- Used for small datasets, \n",
    "- Good as a baseline\n",
    "- Easy to explain\n",
    "- IRIS dataset example\n",
    "- Cancer dataset example\n",
    "- 2 important parameters for k-NN classifier: The number of neighbors and how you measure the distance between points .\n",
    "- Neighbors - 3-5 usually works well. see cancer example plotted as comparison of training and test accuracy as a function of n_neighbors to get a good feel of how to check and know if your distances are predicted accurately.  \n",
    "- Distance - Euclidian is default, it's pretty complex to choose when to change this, and we will not cover it yet.\n",
    "    \n",
    "\n",
    "**Nearest Neighbors Regression**\n",
    "- from sklearn.neighbors import KNeighborsRegressor\n",
    "-  Regressors scores are R^2 (Coefficients of Determination) a measure of goodness of a prediction for a regression model, and yields a score between 0 and 1. 1 = a perfect prediction, 0 = a constant model that just predicts the mean of the training set responses. Closer to 1 is better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models\n",
    "- Go-To as a first algo to try, good for very large datasets and good for high dimensionality (lots of features)\n",
    "\n",
    "### Linear Models for Regression \n",
    "\n",
    "#### Lecture 2 (NOT DONE)\n",
    "\n",
    "#### Ordinary Least Squares\n",
    "\n",
    "- Linear Models make predictions using a Linear Function of the input features.\n",
    "- Similar to the line function. y=mx+b but we have coefficients, and then string them along based on the amount of features we have. The predicted response is the weighted sum of the input features with weights (can be negative) given by w.\n",
    "- Linear models can be characterized as regression models for which the prediction is a line for a single feature. A plane when using 2 features, and a hyperplane in higher dimensions of features. \n",
    "- Ahough we see a line with one feature, it's powerful when you have many features. In fact, if you have MORE features than training data any target y can be perfectly modeled on the training set as a linear function.\n",
    "- Manny linear models for regression, the differences are how the models prameters w and b are learned from the training data, and how we control model complexity. We are only looking at a few models here. \n",
    "- No parameters, but also no way to control model complexity\n",
    "- from sklearn import LinearRegression\n",
    "- Look for an R^2 to determine the performance of the model to your data\n",
    "- Higher features = Higher-Dimensional datasets\n",
    "- Linear models with higher dimensional data become more powerful, but also have a chance at overfitting to the training data.\n",
    "- Boston Housing Example (506 samples, and 105 derived features)\n",
    "\n",
    "#### Ridge Regression\n",
    "- Uses linear regression with ordinary Least Squares.\n",
    "- The coefficients(w) are chosen not only so that they predict well on the training data, but also to fit an additional constraint. We want the magnitude of the coefficients to be as small as possible. i.e. all entries of w should be close to 0. This means each feature shuld have as little effect on the outcome as possible. which translates to having a small slope. while still predicting well. This constraint is an example of Regularization (explicitly restricting a model to avoid overfitting). In this Ridge model we are using the L2 Regularization. Mathematically Ridge penalizes the L2 Norm of the Coeff, or the Euclidian length of w.\n",
    "- If you compare the Linear regression model to ridge LR model. You can notice the less complex model means worse performance on the **training set** but better at **generalizing to real test data**. Therefore Ridge is better than linear regression model bc of this L2 Regularization restrictions. \n",
    "- Tweaking Alpha  in a Ridge Model - The ridge model makes a tradeoff between simplicity of the model (near zero coefficients) and its performance on the training set. We use alpha to specify how much importance the model should place  on simplicity versus training set performance. Default alpha=1.0. Optimum depends on each data set. Increasing Alpha forces coefficients to move more towards zero, thereby decreasing training set performance but it might help generalization. To test this we can plot the stored coefficients, (more on parameter choices later.)\n",
    "\n",
    "#### Lasso Regression\n",
    "- Lasso also restricts the coefficients to be close to zero.\n",
    "- It uses L1 regularization.\n",
    "- Consequences of using L1 regularization is that when using the lasso some coefficients are exactly zero. This means some features are Ignored by the model entirely. This is similar somewhat to feature selection (more on that later in unsupervised tools). But, having some coefficients be exactly zero often makes a model easier to interpret and can reveal some important features of your model. \n",
    "- Boston Housing Example. \n",
    "- Choose Ridge over Lasso, however if you have a large amount of features, and expect only a few to be important use Lasso (i.e. on Boston 105 features only 4 were used in Lasso to get a prediction)\n",
    "- Lasso is easier to interpet than Ridge, bc it only chooses a subset of the features. \n",
    "- You can use scikit-Learn's ElasticNet class which combines the penalties of Lasso and Ridge, but you must adjust both L1 and L2 regularization parameters. This combo works best, but is more tedious.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models for Classification\n",
    "In the binary classification using linear models we make a prediction using a formula that looks very similar to the one for linear regression. But instead of just returning the weighted sum of the features we instead threshold the predicted values at 0. Therefore if the function is smaller than 0 we predict the class as -1, and if the function is larger than 0 we will predict a class of +1. All linear models for classification work this way for their predictions. But, the differences are in the way we find the coefficients (w) and the intercepts (b).\n",
    "\n",
    "For linear function of classification the **decision boundary** is a linear function of the input. (for linear regression the output (Y hat) is a linear function of the features (the line, plane or hyperplane for higher dimensions)). A binary linear classifier is a classifier that separates 2 classes using a line, plane, hyperplane.\n",
    "\n",
    "Linear classifiers differ:\n",
    "- The way that they measure how well a certain combo of coeff and intercepts fit the training data\n",
    "- If and what kind of regularization they use. \n",
    "\n",
    "#### linear SVM and logistical regression\n",
    "The two most common linear models for classification are:\n",
    "- **Logistical Regression**\n",
    "\n",
    "NOTE - Logistical regression is a Linear Model used to CLASSIFY it is NOT LinearRegression (it is confusing terminology)\n",
    "\n",
    "    linear_model.LogisticRegression\n",
    "Uses L2 for Regularization with **c** as a param to define the strength of the regularization.\n",
    "- **Support Vector Machine** (Linear SVMs) (SVC = support vector classifier)\n",
    "\n",
    "        svm.LinearSCV\n",
    "Uses L2 for Regularization with **c** as a param to define the strength of the regularization.\n",
    "\n",
    "##### How to use C to define the regularization and not overfit:\n",
    "- For both logistic regression and SVM we use the param c to define the strengh of the L2 regularization.\n",
    "- The higher c is the less regularization we do\n",
    "- The *higher c is* the models try to fit the *Training Set* as much as possible.\n",
    "- The *lower c is* the harder the models work to find a *coefficient vector (w) that is close to 0*.\n",
    "- using low c values make the algo adjust to the majority of data points\n",
    "- The lower c = more regularization (may miss more training data, but will be more regularized to new data coming in)\n",
    "- using a higher value of c stresses the importance of individual data points and them being classified correctly\n",
    "- a higher value of c can indicate overfitting to the training data, and will not capture the overall layout of the classes that well. \n",
    "\n",
    "In these simple forge and wave low dimensional datasets the lines and planes are just examples to help understand linear models and they seem restrictive and leaving out data points due to the straight line factor, but as you increas the features and climb into high dimensionality they becomes very powerful. They guard against overfitting which is super important as you increase the features.\n",
    "\n",
    "Try LogisticRegression on the Breast Cancer dataset:\n",
    "- import the dataset and load it\n",
    "- run train test split (don't forget that stratify is the target column, and set the random seed state to 42)\n",
    "- create the LogisticRegression object and fit the training X andy data\n",
    "- Print the score on the training data (note an R^2 bc it's a classification not a regression even though it seems that way in the name)\n",
    "- Print the score on the test data\n",
    "- default c = 1 and does NOT need to be added\n",
    "\n",
    "Rerun the breast cancer dataset with a c=100:\n",
    "- same as above, but add c=110 when creating the LogisticRegression Object (logreg100 = LogisticRegression(c=100).fit(X_train, y_train) )\n",
    "\n",
    "The **C=100 results in higher training set accuracy** and *a slightly increased test set accuracy* which corresponds to the idea that a more complex model should perform better.\n",
    "\n",
    "What happens if we go the other way, and use more regularization by dropping the c value from c=1 to c=0.1:\n",
    "- same as above, but add c=0.1 when creating the LogisticRegression Object (logreg100 = LogisticRegression(c=0.01).fit(X_train, y_train) )\n",
    "- Since at 1 we are already in an underfit model, now the training and test data accuracy decreases relative to the default param of c=1\n",
    "- Interpretations of coeff of linear models should always be taken with a grain of salt, as the signs on the coeff can change and be different than the magnitude for different c values, and therefore the class it affects can change as well.\n",
    "\n",
    "\n",
    "What happens if we want to have a more interpretable model with LESS FEATURES:\n",
    "- We can use L1 for the regularization\n",
    "- To Change this we use the PENELTY parameter\n",
    "- lr_l1 = LogisticRegression(C=1, penalty=\"l1\").fit(X_train, y_test)\n",
    "- we can run the logistic regression with the c = 0.001, 1 and 100 with the L1 regularization\n",
    "- The training and test accuracy are almost identical to each other\n",
    "\n",
    "There are many parallels between linear models for BINARY classification and linear models for regression. As in regression the main difference between the various models is the PENALTY parameter. Bc it influences the REGULARIZATION and if the model will use ALL the available features or select only a subset of features to use in it's modeling. Easily visible if you plot the coefficients learned by logistical regression with an L1 penalty on the breast cancer dataset for different values of C (0.001, 1, 100)\n",
    "\n",
    "\n",
    "#### linear models for multiclass classification\n",
    "- Many linear classification models that are binarry classifiers don't translate perfectly to multiclassification, logistical regression does extend into multiclassification. \n",
    "- We have a trick, we can extend a binary model using one-versus-rest.\n",
    "\n",
    "##### One Vs. Rest Approach\n",
    "- we learn a binary linear model for classification for each class\n",
    "- we try to separate the class from the other clasess\n",
    "- we have as many binary models as we have features\n",
    "- To predict we run ALL binary models on a test point\n",
    "- The binary model that has the highest score on it's single class wins, and this class label is returned as the prediction.\n",
    "- Having one binary classifier per class gives us one vector of coeff (w) and one intercept (b) for each class which is SIMILAR to the math done in MULTICLASS LOGISTIC REGRESSION which also ends up with one coeff vetor and one intercept PER CLASS, and then we ultimately use the SAME PREDICTION METHOD.\n",
    "\n",
    "Example: apply the one vs rest method to a 3 class classification dataset. \n",
    "- We use a 2D dataset where each class was sampled from a Gaussian Distribution. \n",
    "- Train a LinearSVC classifier on the dataset\n",
    "- LinearSVC().fit(X, y)\n",
    "- print the coef_.shape from the linear_svm model\n",
    "- print the intercept.shape from the linear_svm model\n",
    "- results in (3,2) for the coeff and the intercept is (3,)\n",
    "- therefore, each row of coef_ contains the coeff vector for one of the three classes and each column hold the coeff value for a specific feature (2 in this dataset). The intercept_ is now a 1-D array storing the intercepts for each class.\n",
    "- NOTE: The trainling underscore intercept_ and coef_ are ways of telling us that the data is stored and created for this model (we can actually go find them in the folder structure, and delete them if we want to start new with the same model object)\n",
    "- plotting the decision boundary for these separate blobs means we have a clear division between them when plotting (a good synthetic example bc of the gaussian blobs originally samapled).\n",
    "\n",
    "Strengths and weaknesses of Linear Models:\n",
    "- main params of linear models are the regularization params called alpha for the regression models and C in LinearSVC and LogisticRegression\n",
    "- large values for alpha or small values for c mean simpler models\n",
    "- for regression models tuning these params are important\n",
    "- C and alpha are searched for on a Logorithmic scale\n",
    "- You aso can choose to use L1 or L2 for regularization\n",
    "- if few features are important to you then use L1, otherwise default to L2\n",
    "- L1 can also be used for easier interpretation of the model if that is important, bc L1 will only use a few features and drop the rest close to zero, and therefore you can easily explain what features have an impact on the model and what effects those features actually are.\n",
    "- Linear models are very fast to train and fast to predict\n",
    "- Linear Models scale to large datasets and work well with sparse datasets (lots of 0's)\n",
    "- If the data has hundreds of thousands or millions of samples consider using solver='sag' options in LogisticRegression and Ridge bc they can be faster than the defaults for large datasets\n",
    "- Otehr options are SGDClassifier class and the SGDRegressor class which implement even more scalable version of the linear models\n",
    "- Linear models make it easy tp understand how a prediction is made bvc we understand the formulas, but the coeff are the confounding factor to explain. Even more true if you have high correlation in your features. \n",
    "- Linear models work well with lots of features (high dimensionality) compared to the samples\n",
    "- often used on large datasets bc it's not feasible to train on other models\n",
    "- low dimension space other models might be better for genralization perfomances. I.e. kernalized Support Vector Machines later.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "- Only for Classification. Even faster than linear models, good for very large datasets and high dimensional datasets. Often less accurate than linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "- Very Fast. Doesn't need scaling of the data, Can be visualized and easily explained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "- Nearly always performs better than a single decision tree, very robust and powerful. Don't need scaling of data. Not good for very high dimensional data that is sparse (lots of 0's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Decision Trees\n",
    "- Often slightly more accurate than random forests. SLower to train but faster to predict than random forests and smaller in memory. Need more parameters tuned than random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "- Powerful for medium sized datasets of features with similar meanings. Requires scaling of data. Sensitive to parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "- Can build very complex models, especially for large datasets. Sensitive to scaling of the data and to the choices of parameters. Large models need a long time to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning and Preprocessing\n",
    "- types of unsupervised learning\n",
    "- challenges in unsupervised learning\n",
    "- preprocessing and scaling\n",
    "- different kinds of preprocessing\n",
    "- applying data transformations\n",
    "- scaling training and test data the same way\n",
    "- the effect of preprocessing on supervised learning\n",
    "- Dimensionality reduction, feature extraction, and manifold learning\n",
    "- principal component alalysis (PCA)\n",
    "- non-negative matrix factorization (NMF)\n",
    "- manifold learning with t-SNE\n",
    "- clustering\n",
    "- k-means clustering\n",
    "- agglomerative clustering\n",
    "- DBSCAN\n",
    "- comparing and evaluating clustering algos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing data and Engineering Features\n",
    "- categorical variables\n",
    "- one hot encoding dummy variables\n",
    "- numbers can encode categorical\n",
    "- binning, discretization, linear models, trees\n",
    "- interactions and polynomials\n",
    "- univariate nonlinear transformations\n",
    "- automatic feature selection\n",
    "- univariate stats\n",
    "- model-based feature selection \n",
    "- interative feature selection\n",
    "- how to use expert knowledge in a domain properly\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Improvement\n",
    "- cross validation\n",
    "- cross validation in scikit learn\n",
    "- benefits of cross validation\n",
    "- stratified k- fold corss validation and other strategies\n",
    "- grid search\n",
    "- simple grid search\n",
    "- the danger of overfitting the params and the validation set\n",
    "- grid search with cross-validation\n",
    "- evaluation metrics and scoring\n",
    "- keep the end goal in mind\n",
    "- metrics for binary classification\n",
    "- metrics for multiclass classification\n",
    "- regression metrics\n",
    "- using evaluation metrics in model selection\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Chains and Pipelines\n",
    "- param selection with preprocessing\n",
    "- building pipelines\n",
    "- using pipelines in grid searchs\n",
    "- convenient pipeline creation with make_pipeline\n",
    "- accessing step attributes\n",
    "- accessing attribules in a pipeline inside GridSearchCV\n",
    "- Grid searching preprocessing steps and model params\n",
    "- grid-searching which model to use\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Data\n",
    "- types of data represented as strings\n",
    "- examples application with sentiment analysis of movie review open data set\n",
    "- representing text data as a bag of words\n",
    "- applying bag-of-words to a toy dataset\n",
    "- bag od words for movie reviews\n",
    "- stopwords\n",
    "- rescaling with data with tf-idf\n",
    "- investigating model coeff\n",
    "- bag of words with more than one word aka n-grams\n",
    "- advanced tokenization, stemming, and lemmatization\n",
    "- topic modeling and document clustering\n",
    "- latent dirichlet allocation\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More examples from ChiHackNight!\n",
    "- Divy Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More...\n",
    "- Kaggle, approach problems, view older work to see how it was done, submit your model to be tested on brand new data\n",
    "- spark, Hadoop and clusters\n",
    "- how to approach a ML problem\n",
    "- differences from inference wth stats packages and ML prediction\n",
    "- when you can fully automate and when to keep humans in the loop\n",
    "- differences between production code model deployment in Scala, Go, C++ or Java versus a python notebook, a/b testing, deploying a model to an api\n",
    "- industrial applications and niche issues (healthcare, satelite imagery, financial)\n",
    "- Finding datasets, scraping datasets, non kaggle competitions\n",
    "- ML abstractions and different ML platforms (fast.ai, tensor flow, keras, microsoft, facebook, ubers, deep brain,...)\n",
    "- best practices, tricks (automatic learning curve, ensembles, etc), \n",
    "- Where to learn more: Coursera Andrew Ng, google crash course, MIT lectures, Udemy, etc\n",
    "- What to learn more: linear algebra, stats, deep neural nets, code, \n",
    "- ChiHackNight webscraping to enhance datasets and combining datasets applications\n",
    "- how to set up your environment for repeatability (env+conda)\n",
    "- how to set up a remote environment with gpu focused testing (aws, papersource, crestle, etc)\n",
    "- How to set up a scalable hadoop environment for big data\n",
    "- Quick facts on NUMPY, PANDAS, etc (see Python For Data Science Cheat Sheet NumPy Basics)\n",
    "- plotly and dash\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_env]",
   "language": "python",
   "name": "conda-env-my_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
